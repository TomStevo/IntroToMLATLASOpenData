{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f164095",
   "metadata": {},
   "source": [
    "<CENTER><img src=\"./ATLASOD.gif\" style=\"width:50%\"></CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911cd8e",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning using ATLAS Open Data\n",
    "\n",
    "This notebook uses a cut-down version of ATLAS Open Data http://opendata.atlas.cern to show you the steps to apply Machine Learning in search for the Higgs boson!\n",
    "\n",
    "ATLAS Open Data provides open access to proton-proton collision data at the LHC for educational purposes. ATLAS Open Data resources are ideal for high-school, undergraduate and postgraduate students.\n",
    "\n",
    "Notebooks are web applications that allow you to create and share documents that can contain for example:\n",
    "1. live code\n",
    "2. visualisations\n",
    "3. narrative text\n",
    "\n",
    "This notebook builds on [HZZAnalysis.ipynb](https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata/blob/master/13-TeV-examples/uproot_python/HZZAnalysis.ipynb) and is adapted from [HZZ_NeuralNet_demo.ipynb](https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata/blob/master/13-TeV-examples/uproot_python/HZZ_NeuralNet_demo.ipynb).\n",
    "\n",
    "HZZAnalysis.ipynb loosely follows the [discovery of the Higgs boson by ATLAS](https://www.sciencedirect.com/science/article/pii/S037026931200857X) (mostly Section 4 and 4.1)\n",
    "\n",
    "Notebooks are a perfect platform to develop Machine Learning for your work, since you'll need exactly those 3 things: code, visualisations and narrative text!\n",
    "\n",
    "We're interested in Machine Learning because we can design an algorithm to figure out for itself how to do various analyses, potentially saving us countless human-hours of design and analysis work.\n",
    "\n",
    "Machine Learning use within ATLAS includes: \n",
    "* particle tracking\n",
    "* particle identification\n",
    "* signal/background classification\n",
    "* and more!\n",
    "\n",
    "This notebook will focus on signal/background classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0096f0",
   "metadata": {},
   "source": [
    "Contents:\n",
    "\n",
    "1. [Running a Jupyter notebook](#Running-a-Jupyter-notebook) <br />\n",
    "2. [First time setup on your computer](#First-time-setup-on-your-computer) <br />\n",
    "3. [To setup everytime](#To-setup-everytime) <br />\n",
    "4. [Samples](#Samples) <br />\n",
    "5. [Reading file with uproot](#Reading-file-with-uproot) <br />\n",
    "6. [Set up the data for the Machine Learning Algorithms](#Set-up-the-data-for-the-Machine-Learning-Algorithms) <br />\n",
    "   6.1 [The Training and Testing split](#The-Training-and-Testing-split) <br />\n",
    "7. [Decision Trees](#Decision-Trees) <br />\n",
    "   7.1 [Receiver Operarting Characteristic (ROC) curve](#Receiver-Operarting-Characteristic-(ROC)-curve) <br />\n",
    "7. [Going-further](#Going-further) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6255e1",
   "metadata": {},
   "source": [
    "## Running a Jupyter notebook\n",
    "\n",
    "To run the whole Jupyter notebook, in the top menu click Cell -> Run All.\n",
    "\n",
    "To propagate a change you've made to a piece of code, click Cell -> Run All Below.\n",
    "\n",
    "You can also run a single code cell, by clicking Cell -> Run Cells, or using the keyboard shortcut Shift+Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658f273",
   "metadata": {},
   "source": [
    "## First time setup on your computer\n",
    "This first cell only needs to be run the first time you open this notebook on your computer. \n",
    "\n",
    "If you close Jupyter and re-open on the same computer, you won't need to run this first cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade --user pip # update the pip package installer\n",
    "!{sys.executable} -m pip install uproot3 pandas numpy matplotlib sklearn --user # install required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a4567",
   "metadata": {},
   "source": [
    "## To setup everytime\n",
    "Cell -> Run All Below\n",
    "\n",
    "to be done every time you re-open this notebook\n",
    "\n",
    "We're going to be using a number of tools to help us:\n",
    "* uproot: lets us read .root files typically used in particle physics into data formats used in Machine Learning\n",
    "* pandas: lets us store data as dataframes, a format widely used in Machine Learning\n",
    "* numpy: provides numerical calculations such as histogramming\n",
    "* matplotlib: common tool for making plots, figures, images, visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a60506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot3 # for reading .root files\n",
    "import pandas as pd # to store data as dataframe\n",
    "import time # to measure time to analyse\n",
    "import math # for mathematical functions such as square root\n",
    "import numpy as np # for numerical calculations such as histogramming\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from matplotlib.ticker import AutoMinorLocator # for minor ticks\n",
    "\n",
    "import infofile # local file containing info on cross-sections, sums of weights, dataset IDs\n",
    "#plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf0837",
   "metadata": {},
   "source": [
    "## Samples\n",
    "\n",
    "In this notebook we only process the signal <span style=\"color:blue\">H->ZZ</span> and the main background <span style=\"color:red\">ZZ</span>, for illustration purposes. You can add data and the Z and ttbar <span style=\"color:red\">backgrounds</span> after if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b86b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "\n",
    "    'ZZ' : {\n",
    "        'list' : ['llll']\n",
    "    },\n",
    "\n",
    "    r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$' : { # H -> ZZ -> llll\n",
    "        'list' : ['ggH125_ZZ4lep'] # gluon-gluon fusion\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a94289",
   "metadata": {},
   "source": [
    "Define function to get data from files. \n",
    "\n",
    "The datasets used in this notebook have already been filtered to include at least 4 leptons per event, so that processing is quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7763552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_files():\n",
    "\n",
    "    data = {} # define empty dictionary to hold dataframes\n",
    "    tuple_path = \"4lep/\" #local\n",
    "    #tuple_path = \"https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/4lep/\" # web address\n",
    "    for s in samples: # loop over samples\n",
    "        print('Processing '+s+' samples') # print which sample\n",
    "        frames = [] # define empty list to hold data\n",
    "        for val in samples[s]['list']: # loop over each file\n",
    "            if s == 'data': prefix = \"Data/\" # Data prefix\n",
    "            else: # MC prefix\n",
    "                prefix = \"/MC/mc_\"+str(infofile.infos[val][\"DSID\"])+\".\"\n",
    "            fileString = tuple_path+prefix+val+\".4lep.root\" # file name to open\n",
    "            temp = read_file(fileString,val) # call the function read_file defined below\n",
    "            frames.append(temp) # append dataframe returned from read_file to list of dataframes\n",
    "        data[s] = pd.concat(frames) # dictionary entry is concatenated dataframes\n",
    "    \n",
    "    return data # return dictionary of dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f24d5",
   "metadata": {},
   "source": [
    "We add functions to return the individual lepton transverse momenta, in GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7124bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lep_pt_i(lep_pt,i):\n",
    "    return lep_pt[i]/1000 # /1000 to go from MeV to GeV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cce0f",
   "metadata": {},
   "source": [
    "## Reading file with uproot\n",
    "If you change anything related to the inputs from the file: Cell -> Run All Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27081a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path,sample):\n",
    "    start = time.time() # start the clock\n",
    "    print(\"\\tProcessing: \"+sample) # print which sample is being processed\n",
    "    data_all = pd.DataFrame() # define empty pandas DataFrame to hold all data for this sample\n",
    "    tree = uproot3.open(path)[\"mini\"] # open the tree called mini\n",
    "    numevents = uproot3.numentries(path, \"mini\") # number of events\n",
    "    for data in tree.iterate(['eventNumber',\n",
    "                              'lep_charge','lep_type','lep_pt',\n",
    "                              # uncomment these variables if you want to calculate masses \n",
    "                              'lep_eta','lep_phi','lep_E', \n",
    "                              # add more variables here if you make cuts on them \n",
    "                              'totalWeight'\n",
    "                             ], # variables to calculate Monte Carlo weight\n",
    "                             outputtype=pd.DataFrame, # choose output type as pandas DataFrame\n",
    "                             entrystop=numevents): # process up to numevents*fraction\n",
    "\n",
    "        # return the individual lepton transverse momenta in GeV\n",
    "        data['lep_pt_1'] = np.vectorize(calc_lep_pt_i)(data.lep_pt,1)\n",
    "        data['lep_pt_2'] = np.vectorize(calc_lep_pt_i)(data.lep_pt,2)\n",
    "        \n",
    "        nIn = len(data.index) # number of events in this batch\n",
    "        nOut = len(data.index) # number of events passing cuts in this batch\n",
    "        data_all = pd.concat([data_all,data]) # append dataframe from this batch to the dataframe for the whole sample\n",
    "        elapsed = time.time() - start # time taken to process\n",
    "        print(\"\\t\\t nIn: \"+str(nIn)+\",\\t nOut: \\t\"+str(nOut)+\"\\t in \"+str(round(elapsed,1))+\"s\") # events before and after\n",
    "    \n",
    "    return data_all # return dataframe containing events passing all cuts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b26f3eb",
   "metadata": {},
   "source": [
    "This is where the processing happens (this will take a few seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de239ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time() # time at start of whole processing\n",
    "data = get_data_from_files() # process all files\n",
    "elapsed = time.time() - start # time after whole processing\n",
    "print(\"Time taken: \"+str(round(elapsed,1))+\"s\") # print total time taken to process every file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5621bf9b",
   "metadata": {},
   "source": [
    "To see what is in the background (ZZ) dataframe run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde4b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['ZZ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbdb22b",
   "metadata": {},
   "source": [
    "Here we define histograms for the variables that we'll look to train the MVAs with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b6a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "lep_pt_2 = { # dictionary containing plotting parameters for the lep_pt_2 histogram\n",
    "    # change plotting parameters\n",
    "    'bin_width':1, # width of each histogram bin\n",
    "    'num_bins':50, # number of histogram bins\n",
    "    'xrange_min':10, # minimum on x-axis\n",
    "    'xlabel':r'$lep\\_pt$[2] [GeV]', # x-axis label\n",
    "}\n",
    "\n",
    "lep_pt_1 = { # dictionary containing plotting parameters for the lep_pt_1 histogram\n",
    "    # change plotting parameters\n",
    "    'bin_width':1, # width of each histogram bin\n",
    "    'num_bins':50, # number of histogram bins\n",
    "    'xrange_min':10, # minimum on x-axis\n",
    "    'xlabel':r'$lep\\_pt$[1] [GeV]', # x-axis label\n",
    "}\n",
    "\n",
    "SoverB_hist_dict = {'lep_pt_2':lep_pt_2,'lep_pt_1':lep_pt_1} \n",
    "# add a histogram here if you want it plotted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296a9aeb",
   "metadata": {},
   "source": [
    "This cell is just to put some nice ATLAS Open Data labels on the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83233760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opendatatext(ax):\n",
    "    # Add text 'ATLAS Open Data' on plot\n",
    "    plt.text(0.05, # x\n",
    "             0.93, # y\n",
    "             'ATLAS Open Data', # text\n",
    "             transform=ax.transAxes, # coordinate system used is that of distributions_axes\n",
    "             fontsize=13 ) \n",
    "    # Add text 'for education' on plot\n",
    "    plt.text(0.05, # x\n",
    "             0.88, # y\n",
    "             'for education', # text\n",
    "             transform=ax.transAxes, # coordinate system used is that of distributions_axes\n",
    "             style='italic',\n",
    "             fontsize=8 )  \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac749ca",
   "metadata": {},
   "source": [
    "Here we define a function to illustrate the optimum cut value on individual variables, based on <span style=\"color:blue\">signal</span> to <span style=\"color:red\">background</span> ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_SoverB(data):\n",
    "    \n",
    "    signal = r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$' # which sample is the signal\n",
    "\n",
    "    # *******************\n",
    "    # general definitions (shouldn't need to change)\n",
    "\n",
    "    for x_variable,hist in SoverB_hist_dict.items(): # access the dictionary of histograms defined in the cell above\n",
    "\n",
    "        h_bin_width = hist['bin_width'] # get the bin width defined in the cell above\n",
    "        h_num_bins = hist['num_bins'] # get the number of bins defined in the cell above\n",
    "        h_xrange_min = hist['xrange_min'] # get the x-range minimum defined in the cell above\n",
    "        h_xlabel = hist['xlabel'] # get the x-axis label defined in the cell above\n",
    "    \n",
    "        bin_edges = [ h_xrange_min + x*h_bin_width for x in range(h_num_bins+1) ] # bin limits\n",
    "        bin_centres = [ h_xrange_min+h_bin_width/2 + x*h_bin_width for x in range(h_num_bins) ] # bin centres\n",
    "        \n",
    "        signal_x = data[signal][x_variable] # histogram the signal\n",
    "    \n",
    "        mc_x = [] # define list to hold the Monte Carlo histogram entries\n",
    "\n",
    "        for s in samples: # loop over samples\n",
    "            if s not in ['data', signal]: # if not data nor signal\n",
    "                mc_x = [*mc_x, *data[s][x_variable] ] # append to the list of Monte Carlo histogram entries\n",
    "\n",
    "    \n",
    "    \n",
    "        # *************\n",
    "        # Signal and background distributions\n",
    "        # *************\n",
    "        distributions_axes = plt.gca() # get current axes\n",
    " \n",
    "        mc_heights = distributions_axes.hist(mc_x, bins=bin_edges, color='red', \n",
    "                                             label='Total background',\n",
    "                                             histtype='step', # lineplot that's unfilled\n",
    "                                             density=True ) # normalize to form probability density\n",
    "        signal_heights = distributions_axes.hist(signal_x, bins=bin_edges, color='blue',\n",
    "                                                 label=signal, \n",
    "                                                 histtype='step', # lineplot that's unfilled\n",
    "                                                 density=True, # normalize to form probability density\n",
    "                                                 linestyle='--' ) # dashed line\n",
    "        \n",
    "        distributions_axes.set_xlim( left=bin_edges[0], right=bin_edges[-1] ) # x-limits of the distributions axes\n",
    "        distributions_axes.set_ylabel('Arbitrary units' ) # y-axis label for distributions axes\n",
    "        distributions_axes.set_ylim( top=max(signal_heights[0])*1.3 ) # set y-axis limits\n",
    "        plt.title('Signal and background '+x_variable+' distributions') # add title\n",
    "        distributions_axes.legend() # draw the legend\n",
    "        distributions_axes.set_xlabel( h_xlabel ) # x-axis label\n",
    "        \n",
    "        opendatatext(ax=plt.gca())\n",
    "    \n",
    "        plt.show() # show the Signal and background distributions\n",
    "    \n",
    "    \n",
    "        # *************\n",
    "        # Signal to background ratio\n",
    "        # *************\n",
    "        plt.figure() # start new figure\n",
    "        SoverB = [] # list to hold S/B values\n",
    "        for cut_value in bin_edges: # loop over bins\n",
    "            signal_weights_passing_cut = sum(data[signal][data[signal][x_variable]<cut_value].totalWeight)\n",
    "            background_weights_passing_cut = 0 # start counter for background weights passing cut\n",
    "            for s in samples: # loop over samples\n",
    "                if s not in ['data', signal]: # if not data nor signal\n",
    "                    background_weights_passing_cut += sum(data[s][data[s][x_variable]<cut_value].totalWeight)\n",
    "            if background_weights_passing_cut!=0: # some background passes cut\n",
    "                SoverB_value = signal_weights_passing_cut/background_weights_passing_cut\n",
    "                SoverB_percent = 100*SoverB_value # multiply by 100 for percentage\n",
    "                SoverB.append(SoverB_percent) # append to list of S/B values\n",
    "        \n",
    "        SoverB_axes = plt.gca() # get current axes\n",
    "        SoverB_axes.plot( bin_edges[:len(SoverB)], SoverB ) # plot the data points\n",
    "        SoverB_axes.set_xlim( left=bin_edges[0], right=bin_edges[-1] ) # set the x-limit of the main axes\n",
    "        SoverB_axes.set_ylabel( 'S/B (%)' ) # write y-axis label for main axes\n",
    "        plt.title('Signal to background ratio for different '+x_variable+' cut values', family='sans-serif')\n",
    "        SoverB_axes.set_xlabel( h_xlabel ) # x-axis label \n",
    "        \n",
    "        plt.show() # show S/B plot\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34932120",
   "metadata": {},
   "source": [
    "Here we call our function to illustrate the optimum cut value on individual variables, based on <span style=\"color:blue\">signal</span> to <span style=\"color:red\">background</span> ratio.\n",
    "\n",
    "We're not doing any Machine Learning yet! We're looking at the variables we'll later use for Machine Learning.\n",
    "\n",
    "Let's talk through the lep_pt_2 plots.\n",
    "1. Imagine placing a cut at 20 GeV in the distributions of <span style=\"color:blue\">signal</span> and <span style=\"color:red\">background</span> (1st plot). This means keeping all events less than 11 GeV in the <span style=\"color:blue\">signal</span> and <span style=\"color:red\">background</span> histograms. \n",
    "2. We then take the ratio of the number of <span style=\"color:blue\">signal</span> events that pass this cut, to the number of <span style=\"color:red\">background</span> events that pass this cut. This gives us a starting value for S/B (2nd plot). \n",
    "3. We then increase this cut value to 12 GeV, 13 GeV, 14 GeV, etc. Cuts at these values are throwing away more <span style=\"color:red\">background</span> than <span style=\"color:blue\">signal</span>, so S/B increases. \n",
    "4. There comes a point around 26 GeV where we start keeping too much <span style=\"color:blue\">background</span>, thus S/B starts to decrease. \n",
    "5. Our goal is to find the maximum in S/B, and place the cut there.\n",
    "\n",
    "The same logic applies to lep_pt_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24923fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_SoverB(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ca98f",
   "metadata": {},
   "source": [
    "In the [ATLAS Higgs discovery paper](https://www.sciencedirect.com/science/article/pii/S037026931200857X), there are a number of numerical cuts applied, not just on lep_pt_1 and lep_pt_2.\n",
    "\n",
    "Imagine having to separately optimise about 7 variables! Not to mention that applying a cut on one variable could change the distribution of another, which would mean you'd have to re-optimise... Nightmare.\n",
    "\n",
    "This is where a Machine Learning algorithms can come to the rescue; they can optimise all variables at the same time.\n",
    "\n",
    "A ML algorithms not only optimises cuts, but can find correlations in many dimensions that will give better signal/background classification than individual cuts ever could.\n",
    "\n",
    "That's the end of the introduction to why one might want to use machine learning. If you'd like to try using one, just keep reading below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85a04c",
   "metadata": {},
   "source": [
    "## Set up the data for the Machine Learning Algorithms\n",
    "\n",
    "Choose variables for use in the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_NN = {} # define empty dictionary to hold dataframes that will be used to train the NN\n",
    "NN_inputs = ['lep_pt_1','lep_pt_2'] # list of features for Neural Network\n",
    "for key in data: # loop over the different keys in the dictionary of dataframes\n",
    "    data_for_NN[key] = data[key][NN_inputs].copy()\n",
    "data_for_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03914466",
   "metadata": {},
   "source": [
    " Organise data ready for the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffde06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sklearn data is usually organised                                                                                                                                           \n",
    "# into one 2D array of shape (n_samples x n_features)                                                                                                                             \n",
    "# containing all the data and one array of categories                                                                                                                             \n",
    "# of length n_samples  \n",
    "\n",
    "all_MC = [] # define empty list that will contain all features for the MC\n",
    "for key in data: # loop over the different keys in the dictionary of dataframes\n",
    "    if key!='data': # only MC should pass this\n",
    "        all_MC.append(data_for_NN[key]) # append the MC dataframe to the list containing all MC features\n",
    "#X = np.concatenate(all_MC) # concatenate the list of MC dataframes into a single 2D array of features, called X\n",
    "X = pd.concat(all_MC)\n",
    "\n",
    "all_y = [] # define empty list that will contain labels whether an event in signal or background\n",
    "for key in data: # loop over the different keys in the dictionary of dataframes\n",
    "    if key!=r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$' and key!='data': # only background MC should pass this\n",
    "        all_y.append(np.zeros(data_for_NN[key].shape[0])) # background events are labelled with 0\n",
    "all_y.append(np.ones(data_for_NN[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$'].shape[0])) # signal events are labelled with 1\n",
    "y = np.concatenate(all_y) # concatenate the list of lables into a single 1D array of labels, called y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56866896",
   "metadata": {},
   "source": [
    "### The Training and Testing split\n",
    "One of the first things to do is split your data into a training and testing set. This will split your data into train-test sets: 90%-10%. It will also shuffle entries so you will not get the first 90% of <span style=\"color:orange\">X</span> for training and the last 33% for testing. This is particularly important in cases where you load all <span style=\"color:blue\">signal</span> events first and then the <span style=\"color:red\">background</span> events.\n",
    "\n",
    "Here we split our data into two independent samples. The split is to create a training and testing set. The first will be used for training the classifier and the second to evaluate its performance.\n",
    "\n",
    "We don't want to test on events that we used to train on, this prevents overfitting to some subset of data so the network would be good for the test data but much worse at any *new* data it sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# make train and test sets\n",
    "X_train,X_test, y_train,y_test = train_test_split(X, y, \n",
    "                                                  test_size=0.1, \n",
    "                                                  random_state=492 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe637d3",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "We'll use SciKit Learn (sklearn) in this tutorial. Other possible tools include keras and pytorch. \n",
    "There will be a few different version to try out; basic Decision Tree classifiers as well as AdaBoost and Gradient Booster Decesion Tree classifiers.\n",
    "\n",
    "Several hyper-parameters are set to non default values, but it is suggested that you play around with some of the options and see how it affects the results.\n",
    "\n",
    "First let's import some of the classifiers we need from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3faf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb1177",
   "metadata": {},
   "source": [
    "The training of a basic Decision Tree classifier with a maximum depth of 3 is done in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb577958",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier(max_depth=3)\n",
    "dt.fit(X_train.values,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc2c135",
   "metadata": {},
   "source": [
    "We are then able to plot the \"decisions\" made at each branching point and see what type of node they result in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(dt,feature_names=['x','y'],class_names=['sig','bkg'],filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a401faf",
   "metadata": {},
   "source": [
    "### Receiver Operarting Characteristic (ROC) curve\n",
    "A useful plot to judge the performance of a classifier is to look at the ROC curve directly.\n",
    "\n",
    "This compares the true positive rate (signal classified as signal) to the false positve rate (background classified as signal). The better the performance of the classifier, the closer to the top left the curve will be. A random guess for two would produce the diagonal dashed line on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881671df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_roc(clf,X,y):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    y_score = clf.predict_proba(X.values)[:,1]\n",
    "    \n",
    "    fpr, tpr, _  = roc_curve(y_test,y_score)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    lw = 2\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=\"darkorange\",\n",
    "        lw=lw,\n",
    "        label=\"ROC curve (area = %0.2f)\" % roc_auc,\n",
    "    )\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver operating characteristic\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    opendatatext(plt.gca())\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e315c1",
   "metadata": {},
   "source": [
    "We can plot the ROC curve for the trained Decision Tree classifier with the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_roc(dt,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb68f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mva(clf,X,y,var1,var2,xmin,xmax):\n",
    "    sig = pd.DataFrame()\n",
    "    bkg = pd.DataFrame()\n",
    "    sig[NN_inputs] = [i for i,j in zip(X.values,y) if j==1.0]\n",
    "    bkg[NN_inputs] = [i for i,j in zip(X.values,y) if j==0.0]\n",
    "    sigsamp = sig.sample(n=200)\n",
    "    bkgsamp = bkg.sample(n=200)\n",
    "    \n",
    "    plt.plot(bkgsamp[var1],bkgsamp[var2], 'o', c='tab:orange', label='bkg', alpha=0.5, markeredgecolor='k')\n",
    "    plt.plot(sigsamp[var1],sigsamp[var2], 'o', c='tab:blue', label='sig', alpha=0.5, markeredgecolor='k')\n",
    "    plt.xlim(xmin,xmax)\n",
    "    plt.ylim(xmin,xmax)\n",
    "    ax = plt.gca()\n",
    "    from sklearn.inspection import DecisionBoundaryDisplay\n",
    "    DecisionBoundaryDisplay.from_estimator(clf, X.values, ax=ax, cmap=plt.cm.PuOr, alpha=0.8, eps=0.5, response_method=\"predict_proba\")\n",
    "    opendatatext(ax)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f3b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_mva(dt,X_test,y_test,'lep_pt_1','lep_pt_2',10,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd1704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABDT = AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=4),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=20)\n",
    "ABDT.fit(X_train.values,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_mva(ABDT,X_test,y_test,'lep_pt_1','lep_pt_2',10,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa06c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorVsTree(clf,X_train,y_train,X_test,y_test):\n",
    "    train_errors = []\n",
    "    test_errors  = []\n",
    "    \n",
    "    for train_predict,test_predict in zip(clf.staged_predict(X_train.values),clf.staged_predict(X_test.values)):\n",
    "        train_errors.append(1. - accuracy_score(train_predict, y_train))\n",
    "        test_errors.append(1. - accuracy_score(test_predict, y_test))\n",
    "\n",
    "    n_trees = len(clf)\n",
    "\n",
    "    plt.plot(range(1, n_trees + 1), train_errors, c='red', label='Train')\n",
    "    plt.plot(range(1, n_trees + 1), test_errors, c='blue', label='Test')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylim(0.9*min(min(train_errors),min(test_errors)),1.1*max(max(train_errors),max(test_errors)))\n",
    "    plt.ylabel('Test Error')\n",
    "    plt.xlabel('Number of Trees')\n",
    "    opendatatext(plt.gca())\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8203ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "errorVsTree(ABDT,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6369f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_roc(ABDT,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb913d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBDT = GradientBoostingClassifier(max_depth=4,learning_rate=0.1,\n",
    "                               n_estimators=20,random_state=0)\n",
    "GBDT.fit(X_train.values,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680862fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "errorVsTree(GBDT,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f189ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_mva(GBDT,X_test,y_test,'lep_pt_1','lep_pt_2',10,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_roc(GBDT,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d49d9b4",
   "metadata": {},
   "source": [
    "### Data Preprocessing for Neural Network\n",
    "\n",
    "The neural network in Python may have difficulty converging before the maximum number of iterations allowed if the data is not standardised. Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. Note that you must apply the same scaling to the test set for meaningful results. There are a lot of different methods for standardization of data, we will use the built-in StandardScaler this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() # initialise StandardScaler\n",
    "\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "# Now apply the transformations to the data:\n",
    "scaled_X_train = pd.DataFrame()\n",
    "scaled_X_test = pd.DataFrame()\n",
    "scaled_X = pd.DataFrame()\n",
    "scaled_X_train[NN_inputs] = scaler.transform(X_train[NN_inputs])\n",
    "scaled_X_test[NN_inputs] = scaler.transform(X_test[NN_inputs])\n",
    "scaled_X[NN_inputs] = scaler.transform(X[NN_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5acdb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "hidden_layer_sizes = [3] # 1 hidden layer\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(hidden_layer_sizes), # define parameters for our multi-layer-perceptron\n",
    "                    max_iter=200 )# max number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61db231",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(scaled_X_train.values,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32da489",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_mva(mlp,scaled_X_test,y_test,'lep_pt_1','lep_pt_2',-2.0,2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed0e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_roc(mlp,scaled_X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff2116",
   "metadata": {},
   "source": [
    "## BDTs and NNs - Going further\n",
    "\n",
    "If you want to go further, there are a number of things you could try:\n",
    "* Modify some hyper-parameters for the various trainings.\n",
    "* Add some more variables into the classifiers. Add them in one at a time, rather than all at once, because adding a variable could decrease performance, due to anti-correlation. For some ideas of variables, you can look at the paper for the [discovery of the Higgs boson by ATLAS](https://www.sciencedirect.com/science/article/pii/S037026931200857X) (mostly Section 4 and 4.1).\n",
    "\n",
    "With each change, keep an eye on the:\n",
    "* total area under the ROC curve, \n",
    "* separation between <span style=\"color:blue\">signal</span> and <span style=\"color:red\">background</span> in the Neural Network output distribution\n",
    "* S/B scores that can be achieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f298f9ba",
   "metadata": {},
   "source": [
    "## General Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afeaaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output(clf,X,y):\n",
    "    n_bins = 40\n",
    "    \n",
    "    sig = pd.DataFrame()\n",
    "    bkg = pd.DataFrame()\n",
    "    \n",
    "    sig[NN_inputs] = [i for i,j in zip(X.values,y) if j==1.0]\n",
    "    bkg[NN_inputs] = [i for i,j in zip(X.values,y) if j==0.0]\n",
    "    \n",
    "    sig_output = clf.predict_proba(sig.values)[:,1]\n",
    "    bkg_output = clf.predict_proba(bkg.values)[:,1]\n",
    "    \n",
    "    d_min = min(sig_output.min(),bkg_output.min())\n",
    "    d_max = max(sig_output.max(),bkg_output.max())\n",
    "    \n",
    "    plt.hist(bkg_output,bins=n_bins,range=(d_min,d_max), color='tab:orange', label='bkg train',alpha=0.6, density=True)\n",
    "    plt.hist(sig_output,bins=n_bins,range=(d_min,d_max), color='tab:blue', label='sig train', alpha=0.6, density=True)\n",
    "    opendatatext(plt.gca())\n",
    "    plt.ylim([0,plt.gca().get_ylim()[1]*1.25])\n",
    "    #print(plt.gca().get_ylim())\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb27fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(mlp,scaled_X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb91b665",
   "metadata": {},
   "source": [
    "### Overtraining Checks\n",
    "Comparing the ML algorithm's output distribution for the training and testing set is a popular way in HEP to check for overtraining. The <span style=\"color:orange\">plot_compare_outputs()</span> method will plot the shape of the algorithm's decision function for each class, as well as overlaying it with the decision function in the training set.\n",
    "\n",
    "There are techniques to prevent overtraining.\n",
    "The output also performs the two-sample Kolmogorov-Smirnov test for goodness of fit between the training and testing samples for both the signal and background.\n",
    "The idea is to compare the underlying distributions of two independent samples, the training and the testing, and if the KS statistic is large, then the p-value will be small, and this suggests that the two do not match well and therefore there may be evidence of overtraining in the classifier.\n",
    "\n",
    "To try and see and example of this go back to one of the BDTs and increase the depth or the number of estimators, and retrain to see how the output distributions compare for different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_outputs(clf,X_train,y_train,X_test,y_test):\n",
    "    from scipy.stats import ks_2samp\n",
    "    n_bins = 30\n",
    "    \n",
    "    sig_train = pd.DataFrame()\n",
    "    bkg_train = pd.DataFrame()\n",
    "    sig_test = pd.DataFrame()\n",
    "    bkg_test = pd.DataFrame()\n",
    "    sig_train[NN_inputs] = [i for i,j in zip(X_train.values,y_train) if j==1.0]\n",
    "    bkg_train[NN_inputs] = [i for i,j in zip(X_train.values,y_train) if j==0.0]\n",
    "    sig_test[NN_inputs] = [i for i,j in zip(X_test.values,y_test) if j==1.0]\n",
    "    bkg_test[NN_inputs] = [i for i,j in zip(X_test.values,y_test) if j==0.0]\n",
    "    \n",
    "    sig_train_output = clf.predict_proba(sig_train.values)[:,1]\n",
    "    bkg_train_output = clf.predict_proba(bkg_train.values)[:,1]\n",
    "\n",
    "    sig_test_output = clf.predict_proba(sig_test.values)[:,1]\n",
    "    bkg_test_output = clf.predict_proba(bkg_test.values)[:,1]\n",
    "    \n",
    "    d_min = min(sig_train_output.min(),bkg_train_output.min())\n",
    "    d_max = max(sig_train_output.max(),bkg_train_output.max())\n",
    "    \n",
    "    sig_tr,bins,_ = plt.hist(bkg_train_output,bins=n_bins,range=(d_min,d_max), color='tab:orange', label='bkg train',alpha=0.6, density=True)\n",
    "    bkg_tr,_,_ = plt.hist(sig_train_output,bins=n_bins,range=(d_min,d_max), color='tab:blue', label='sig train', alpha=0.6, density=True)\n",
    "\n",
    "    bin_centers = (bins[:-1]+bins[1:])/2\n",
    "    sig_te,_ = np.histogram(sig_test_output,bins=bins,density=True)\n",
    "    bkg_te,_ = np.histogram(bkg_test_output,bins=bins,density=True)\n",
    "\n",
    "    plt.plot(bin_centers,bkg_te, 'o', c='tab:orange', label='bkg test', alpha=0.9, markeredgecolor='k')\n",
    "    plt.plot(bin_centers,sig_te, 'o', c='tab:blue', label='sig test', alpha=0.9, markeredgecolor='k')\n",
    "    opendatatext(ax=plt.gca())\n",
    "    plt.ylim([0,plt.gca().get_ylim()[1]*1.4])\n",
    "    print(\"Signal:\",ks_2samp(sig_tr,sig_te))\n",
    "    print(\"Background:\",ks_2samp(bkg_tr,bkg_te))\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10853f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_compare_outputs(mlp,scaled_X_train,y_train,scaled_X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d01a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_compare_outputs(ABDT,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417fbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd0900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorVsSize(clf,cv,X,y,njobs,train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    from sklearn.utils import shuffle\n",
    "    from sklearn.model_selection import learning_curve\n",
    "    \n",
    "    X,y = shuffle(X,y)\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(clf, X, y, cv=cv, n_jobs=njobs, train_sizes=train_sizes)\n",
    "    \n",
    "    train_errors_mean = np.mean(train_scores, axis=1)\n",
    "    train_errors_std = np.std(train_scores, axis=1)\n",
    "    test_errors_mean = np.mean(test_scores, axis=1)\n",
    "    test_errors_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Training Samples\")\n",
    "    plt.ylabel(\"1-Error\")\n",
    "\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_errors_mean - train_errors_std,\n",
    "                     train_errors_mean + train_errors_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_errors_mean - test_errors_std,\n",
    "                     test_errors_mean + test_errors_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_errors_mean, 'o-', color=\"r\",\n",
    "             label=\"Training Error\")\n",
    "    plt.plot(train_sizes, test_errors_mean, 'o-', color=\"g\",\n",
    "             label=\"Test Error\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d35d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "ABDT = AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=6),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=20)\n",
    "\n",
    "cv = ShuffleSplit(n_splits=4, test_size=0.2, random_state=0)\n",
    "errorVsSize(ABDT,cv,X_train,y_train,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec5dc6",
   "metadata": {},
   "source": [
    "## General Techniques - Going further\n",
    "\n",
    "If you want to go further, there are a number of things you could try:\n",
    "* Try comparing the different ML algorithms with different numbers of folds for the cross-validation.\n",
    "* Add in the other <span style=\"color:blue\">H->ZZ signal</span> samples in '[Samples](#samples)'. You can copy them from [HZZAnalysis.ipynb](https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata/blob/master/13-TeV-examples/uproot_python/HZZAnalysis.ipynb) and will need to change the `get_data_from_files()` function to read the web-based files. Try adding them one at a time first, then see how things look with all added.\n",
    "* Add in real data in '[Samples](#samples)' and see whether the Neural Network output distributions in data and simulation match. You can copy data from [HZZAnalysis.ipynb](https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata/blob/master/13-TeV-examples/uproot_python/HZZAnalysis.ipynb) and again will need to change to read the web-based files. \n",
    "\n",
    "Again with each change, keep an eye on the:\n",
    "* total area under the ROC curve, \n",
    "* separation between <span style=\"color:blue\">signal</span> and <span style=\"color:red\">background</span> in the Neural Network output distribution\n",
    "* S/B scores that can be achieved\n",
    "\n",
    "Notice that we've trained and tested our ML algortihms on simulated data. We would then *apply* it to real experimental data. Once you're happy with classifiers, you may want to put it back into a full analysis to run over all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d341e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
